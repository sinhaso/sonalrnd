On Windows


mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\primary\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node2\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node1\config\mongod.conf



mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\primary\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node2\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node1\config\mongod.conf



mongod --config D:\mongoCluster\configServer\configdb1\config\mongod.conf
mongod --config D:\mongoCluster\configServer\configdb2\config\mongod.conf
mongod --config D:\mongoCluster\configServer\configdb3\config\mongod.conf

mongos --config D:\mongoCluster\mongoSServer_Routers\QueryRouter1\config\mongos.conf
mongos --config D:\mongoCluster\mongoSServer_Routers\QueryRouter2\config\mongos.conf


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Backup and Restore :--

mongodump -d MyAppDB -o D:\mongoDataBackup  --host=172.18.128.79 --port=27000


mongorestore --host=172.18.128.79 --port=27000 --drop -d MyAppDB D:\mongoDataBackup\MyAppDB 

Note:-- Make sure to disable cluster config from the mongod isntance in which we want to restore the Data from mongodump
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--> One time Job
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--> Configure ReplicaSet
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\primary\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node2\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node1\config\mongod.conf
mongod --config D:\mongoCluster\shards\shard1\replSet_Shard1\arbiter\config\mongod.conf 



mongo 172.18.128.79:27000
rs.initiate(); // To Make it primary

// Add all secondary and arbiter
rs.add('172.18.128.79:27001');
rs.add('172.18.128.79:27002');
rs.addArb('172.18.128.79:27003');

{
        "info2" : "no configuration explicitly specified -- making one",
        "me" : "172.18.128.79:27000",
        "ok" : 1
}

{ "ok" : 1 }

{ "ok" : 1 }

{ "ok" : 1 }


replSet_Shard1:PRIMARY> rs.conf();
{
        "_id" : "replSet_Shard1",
        "version" : 6,
        "members" : [
                {
                        "_id" : 0,
                        "host" : "172.18.128.79:27000",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : 0,
                        "votes" : 1
                },
                {
                        "_id" : 1,
                        "host" : "172.18.128.79:27001",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : 0,
                        "votes" : 1
                },
                {
                        "_id" : 2,
                        "host" : "172.18.128.79:27002",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : 0,
                        "votes" : 1
                },
                {
                        "_id" : 3,
                        "host" : "172.18.128.79:27003",
                        "arbiterOnly" : true,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : 0,
                        "votes" : 1
                }
        ],
        "settings" : {
                "chainingAllowed" : true,
                "heartbeatTimeoutSecs" : 10,
                "getLastErrorModes" : {

                },
                "getLastErrorDefaults" : {
                        "w" : 1,
                        "wtimeout" : 0
                }
        }
}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--> Configure Query Router :-
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

mongod --config D:\mongoCluster\configServer\configdb1\config\mongod.conf
mongod --config D:\mongoCluster\configServer\configdb2\config\mongod.conf
mongod --config D:\mongoCluster\configServer\configdb3\config\mongod.conf

mongos --config D:\mongoCluster\mongoSServer_Routers\QueryRouter1\config\mongos.conf

mongo 172.18.128.79:27041

mongos> sh.addShard("replSet_Shard1/172.18.128.79:27000");

{ "shardAdded" : "replSet_Shard1", "ok" : 1 }

sh.enableSharding("MyAppDB");
{ "ok" : 1 }		


mongos> sh.status();
--- Sharding Status ---
  sharding version: {
        "_id" : 1,
        "minCompatibleVersion" : 5,
        "currentVersion" : 6,
        "clusterId" : ObjectId("55dbb4c7e2c8ace111860c90")
}
  shards:
        {  "_id" : "replSet_Shard1",  "host" : "replSet_Shard1/172.18.128.79:27000,172.18.128.79:27001,172.18.128.79:27002" }
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                No recent migrations
  databases:
        {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
        {  "_id" : "MyAppDB",  "partitioned" : true,  "primary" : "replSet_Shard1" }
        {  "_id" : "test",  "partitioned" : false,  "primary" : "replSet_Shard1" }
 


mongos>  db.isMaster() ;
{
        "ismaster" : true,
        "msg" : "isdbgrid",
        "maxBsonObjectSize" : 16777216,
        "maxMessageSizeBytes" : 48000000,
        "maxWriteBatchSize" : 1000,
        "localTime" : ISODate("2015-08-25T00:27:27.368Z"),
        "maxWireVersion" : 3,
        "minWireVersion" : 0,
        "ok" : 1
} 


-----------------------------------------------------------------------------------------------------------------------------

mongos --config D:\mongoCluster\mongoSServer_Routers\QueryRouter2\config\mongos.conf

// This mongos will pick all pervious configuration from config servers
 
mongos> sh.status()
--- Sharding Status ---
  sharding version: {
        "_id" : 1,
        "minCompatibleVersion" : 5,
        "currentVersion" : 6,
        "clusterId" : ObjectId("55dbb4c7e2c8ace111860c90")
}
  shards:
        {  "_id" : "replSet_Shard1",  "host" : "replSet_Shard1/172.18.128.79:27000,172.18.128.79:27001,172.18.128.79:27002" }
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                No recent migrations
  databases:
        {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
        {  "_id" : "MyAppDB",  "partitioned" : true,  "primary" : "replSet_Shard1" }
        {  "_id" : "test",  "partitioned" : false,  "primary" : "replSet_Shard1" }
		
		
mongos> db.isMaster();
{
        "ismaster" : true,
        "msg" : "isdbgrid",
        "maxBsonObjectSize" : 16777216,
        "maxMessageSizeBytes" : 48000000,
        "maxWriteBatchSize" : 1000,
        "localTime" : ISODate("2015-08-25T00:42:41.802Z"),
        "maxWireVersion" : 3,
        "minWireVersion" : 0,
        "ok" : 1
}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Config Server's mongo.conf
------------------------------------------------------------------------------------

configsvr=true
dbpath=D:\mongoCluster\configServer\configdb1\mongoData
logpath=D:\mongoCluster\configServer\configdb1\logs\mongod.log
logappend=true
bind_ip = 172.18.128.79
port=27031
------------------------------------------------------------------------------------

configsvr=true
dbpath=D:\mongoCluster\configServer\configdb2\mongoData
logpath=D:\mongoCluster\configServer\configdb2\logs\mongod.log
logappend=true
bind_ip = 172.18.128.79
port=27032


------------------------------------------------------------------------------------
configsvr=true
dbpath=D:\mongoCluster\configServer\configdb3\mongoData
logpath=D:\mongoCluster\configServer\configdb3\logs\mongod.log
logappend=true
bind_ip = 172.18.128.79
port=27033
------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Shard And Replica Set  mongo.conf
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

dbpath=D:\mongoCluster\shards\shard1\replSet_Shard1\primary\mongoData
logpath=D:\mongoCluster\shards\shard1\replSet_Shard1\primary\log\mongod.log
logappend=true
nojournal = false
shardsvr=true
port = 27000
bind_ip = 172.18.128.79
replSet =  replSet_Shard1
rest=true
------------------------------------------------------------------------------------
dbpath=D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node1\mongoData
logpath=D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node1\log\mongod.log
logappend=true
nojournal = false
shardsvr=true
port = 27001
bind_ip = 172.18.128.79
replSet =  replSet_Shard1
rest=true

------------------------------------------------------------------------------------
dbpath=D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node2\mongoData
logpath=D:\mongoCluster\shards\shard1\replSet_Shard1\secondary\node2\log\mongod.log
logappend=true
nojournal = false
shardsvr=true
port = 27002
bind_ip = 172.18.128.79
replSet =  replSet_Shard1
rest=true

------------------------------------------------------------------------------------

dbpath=D:\mongoCluster\shards\shard1\replSet_Shard1\arbiter\mongoData
logpath=D:\mongoCluster\shards\shard1\replSet_Shard1\arbiter\log\mongod.log
logappend=true
nojournal = false
shardsvr=true
port = 27003
bind_ip = 172.18.128.79
replSet =  replSet_Shard1
rest=true
oplogSize=1

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Query Router  mongos.conf
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------

configdb=172.18.128.79:27031,172.18.128.79:27032,172.18.128.79:27033
logpath=D:\mongoCluster\mongoSServer_Routers\QueryRouter1\log\mongos.log
logappend=true
port=27041
bind_ip = 172.18.128.79

------------------------------------------------------------------------------------

configdb=172.18.128.79:27031,172.18.128.79:27032,172.18.128.79:27033
logpath=D:\mongoCluster\mongoSServer_Routers\QueryRouter2\log\mongos.log
logappend=true
port=27042
bind_ip = 172.18.128.79


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


Config Servers

Config servers are special mongod instances that store the metadata for a sharded cluster.

A production sharded cluster has exactly three config servers. All config servers must be available to deploy a sharded cluster or to make any changes to cluster metadata. Config servers do not run as replica sets.

For testing purposes you may deploy a cluster with a single config server. But to ensure redundancy and safety in production, you should always use three.

WARNING
If your cluster has a single config server, then the config server is a single point of failure. If the config server is inaccessible, the cluster is not accessible. If you cannot recover the data on a config server, the cluster will be inoperable.

Always use three config servers for production deployments.
Each sharded cluster must have its own config servers. Do not use the same config servers for different sharded clusters.

TIP
Use CNAMEs to identify your config servers to the cluster so that you can rename and renumber your config servers without downtime.
Read and Write Operations on Config Servers

Config servers store the cluster’s metadata in the config database. The mongos instances cache this data and use it to route reads and writes to shards.

MongoDB only writes data to the config server when the metadata changes, such as

after a chunk migration, or
after a chunk split.
When writing to the three config servers, a coordinator dispatches the same write commands to the three config servers and collects the results. Differing results indicate an inconsistent writes to the config servers and may require manual intervention. Once the config servers become inconsistent, the balancer will not perform any chunk migration and mongos will not perform auto-splits of chunks.

MongoDB reads data from the config server in the following cases:

A new mongos starts for the first time, or an existing mongos restarts.
After change in the cluster metadata, such as after a chunk migration.
MongoDB also uses the config server to manage distributed locks.

Config Server Availability

If one or two config servers become unavailable, the cluster’s metadata becomes read only. You can still read and write data from the shards, but no chunk migrations or splits will occur until all three servers are available.

If all three config servers are unavailable, you can still use the cluster if you do not restart the mongos instances until after the config servers are accessible again. If you restart the mongos instances before the config servers are available, the mongos will be unable to route reads and writes.

Clusters become inoperable without the cluster metadata. To ensure that the config servers remain available and intact, backups of config servers are critical. The data on the config server is small compared to the data stored in a cluster, and the config server has a relatively low activity load. These properties facilitate finding a window to back up the config servers.

If the name or address that a sharded cluster uses to connect to a config server changes, you must restart every mongod and mongos instance in the sharded cluster. Avoid downtime by using CNAMEs to identify config servers within the MongoDB deployment.


-----

Sharded Cluster Query Routing


MongoDB mongos instances route queries and write operations to shards in a sharded cluster. mongos provide the only interface to a sharded cluster from the perspective of applications. Applications never connect or communicate directly with the shards.

The mongos tracks what data is on which shard by caching the metadata from the config servers. The mongos uses the metadata to route operations from applications and clients to the mongod instances. A mongos has no persistent state and consumes minimal system resources.

The most common practice is to run mongos instances on the same systems as your application servers, but you can maintain mongos instances on the shards or on other dedicated resources.


How mongos Determines which Shards Receive a Query
A mongos instance routes a query to a cluster by:

Determining the list of shards that must receive the query.
Establishing a cursor on all targeted shards.


In some cases, when the shard key or a prefix of the shard key is a part of the query, the mongos can route the query to a subset of the shards. Otherwise, the mongos must direct the query to all shards that hold documents for that collection.


Broadcast Operations and Targeted Operations

In general, operations in a sharded environment are either:

Broadcast to all shards in the cluster that hold documents in a collection
Targeted at a single shard or a limited group of shards, based on the shard key
For best performance, use targeted operations whenever possible. While some operations must broadcast to all shards, you can ensure MongoDB uses targeted operations whenever possible by always including the shard key.


Replica Set Arbiter

An arbiter does not have a copy of data set and cannot become a primary. Replica sets may have arbiters to add a vote in elections of for primary. Arbiters always have exactly 1 vote election, and thus allow replica sets to have an uneven number of members, without the overhead of a member that replicates data.

IMPORTANT
Do not run an arbiter on systems that also host the primary or the secondary members of the replica set.
Only add an arbiter to sets with even numbers of members. If you add an arbiter to a set with an odd number of members, the set may suffer from tied elections. To add an arbiter, see Add an Arbiter to Replica Set.




Replica Set High Availability

Replica sets provide high availability using automatic failover. Failover allows a secondary member to become primary if primary is unavailable. Failover, in most situations does not require manual intervention.

Replica set members keep the same data set but are otherwise independent. If the primary becomes unavailable, the replica set holds an election to select a new primary. In some situations, the failover process may require a rollback. [1]

The deployment of a replica set affects the outcome of failover situations. To support effective failover, ensure that one facility can elect a primary if needed. Choose the facility that hosts the core application systems to host the majority of the replica set. Place a majority of voting members and all the members that can become primary in this facility. Otherwise, network partitions could prevent the set from being able to form a majority.

[1]	Replica sets remove “rollback” data when needed without intervention. Administrators must apply or discard rollback data manually.
Failover Processes

The replica set recovers from the loss of a primary by holding an election. Consider the following:

Replica Set Elections
Elections occur when the primary becomes unavailable and the replica set members autonomously select a new primary.
Rollbacks During Replica Set Failover
A rollback reverts write operations on a former primary when the member rejoins the replica set after a failover.










Replica Set Elections

Replica sets use elections to determine which set member will become primary. Elections occur after initiating a replica set, and also any time the primary becomes unavailable. The primary is the only member in the set that can accept write operations. If a primary becomes unavailable, elections allow the set to recover normal operations without manual intervention. Elections are part of the failover process.

In the following three-member replica set, the primary is unavailable. The remaining secondaries hold an election to choose a new primary.

Diagram of an election of a new primary. In a three member replica set with two secondaries, the primary becomes unreachable. The loss of a primary triggers an election where one of the secondaries becomes the new primary
Behavior

Elections are essential for independent operation of a replica set; however, elections take time to complete. While an election is in process, the replica set has no primary and cannot accept writes and all remaining members become read-only. MongoDB avoids elections unless necessary.

If a majority of the replica set is inaccessible or unavailable, the replica set cannot accept writes and all remaining members become read-only.

Factors and Conditions that Affect Elections

Heartbeats
Replica set members send heartbeats (pings) to each other every two seconds. If a heartbeat does not return within 10 seconds, the other members mark the delinquent member as inaccessible.

Priority Comparisons
The priority setting affects elections. Members will prefer to vote for members with the highest priority value.

Members with a priority value of 0 cannot become primary and do not seek election. For details, see Priority 0 Replica Set Members.

A replica set does not hold an election as long as the current primary has the highest priority value or no secondary with higher priority is within 10 seconds of the latest oplog entry in the set.

If a higher-priority member catches up to within 10 seconds of the latest oplog entry of the current primary, the set holds an election in order to provide the higher-priority node a chance to become primary.

Optime
The optime is the timestamp of the last operation that a member applied from the oplog. A replica set member cannot become primary unless it has the highest (i.e. most recent) optime of any visible member in the set.

Connections
A replica set member cannot become primary unless it can connect to a majority of the members in the replica set. For the purposes of elections, a majority refers to the total number of votes, rather than the total number of members.

If you have a three-member replica set, where every member has one vote, the set can elect a primary as long as two members can connect to each other. If two members are unavailable, the remaining member remains a secondary because it cannot connect to a majority of the set’s members. If the remaining member is a primary and two members become unavailable, the primary steps down and becomes a secondary.

Network Partitions
Network partitions affect the formation of a majority for an election. If a primary steps down and neither portion of the replica set has a majority the set will not elect a new primary. The replica set becomes read-only.

To avoid this situation, place a majority of instances in one data center and a minority of instances in any other data centers combined.

Election Mechanics

Election Triggering Events
Replica sets hold an election any time there is no primary. Specifically, the following:

the initiation of a new replica set.
a secondary loses contact with a primary. Secondaries call for elections when they cannot see a primary.
a primary steps down.
NOTE
Priority 0 members, do not trigger elections, even when they cannot connect to the primary.
A primary will step down:

after receiving the replSetStepDown command.
if one of the current secondaries is eligible for election and has a higher priority.
if primary cannot contact a majority of the members of the replica set.
In some cases, modifying a replica set’s configuration will trigger an election by modifying the set so that the primary must step down.

IMPORTANT
When a primary steps down, it closes all open client connections, so that clients don’t attempt to write data to a secondary. This helps clients maintain an accurate view of the replica set and helps prevent rollbacks.
Participation in Elections
Every replica set member has a priority that helps determine its eligibility to become a primary. In an election, the replica set elects an eligible member with the highest priority value as primary. By default, all members have a priority of 1 and have an equal chance of becoming primary. In the default, all members also can trigger an election.

You can set the priority value to weight the election in favor of a particular member or group of members. For example, if you have a geographically distributed replica set, you can adjust priorities so that only members in a specific data center can become primary.

The first member to receive the majority of votes becomes primary. By default, all members have a single vote, unless you modify the votes setting. Non-voting members have votes value of 0. All other members have 1 vote.

Changed in version 3.0.0: Members cannot have votes greater than 1. For details, see Replica Set Configuration Validation.

The state of a member also affects its eligibility to vote. Only members in the following states can vote: PRIMARY, SECONDARY, RECOVERING, ARBITER, and ROLLBACK.

IMPORTANT
Do not alter the number of votes in a replica set to control the outcome of an election. Instead, modify the priority value.
Vetoes in Elections
All members of a replica set can veto an election, including non-voting members. A member will veto an election:

If the member seeking an election is not a member of the voter’s set.
If the member seeking an election is not up-to-date with the most recent operation accessible in the replica set.
If the member seeking an election has a lower priority than another member in the set that is also eligible for election.
If a priority 0 member [1] is the most current member at the time of the election. In this case, another eligible member of the set will catch up to the state of this secondary member and then attempt to become primary.
If the current primary has more recent operations (i.e. a higher optime) than the member seeking election, from the perspective of the voting member.
If the current primary has the same or more recent operations (i.e. a higher or equal optime) than the member seeking election.
[1]	Remember that hidden and delayed imply priority 0 configuration.
Non-Voting Members

Non-voting members hold copies of the replica set’s data and can accept read operations from client applications. Non-voting members do not vote in elections, but can veto an election and become primary.

Because a replica set can have up to 50 members, but only 7 voting members, non-voting members allow a replica set to have more than seven members.

For instance, the following nine-member replica set has seven voting members and two non-voting members.

Diagram of a 9 member replica set with the maximum of 7 voting members.
A non-voting member has a votes setting equal to 0 in its member configuration:

{
  "_id" : <num>
  "host" : <hostname:port>,
  "votes" : 0
}
IMPORTANT
Do not alter the number of votes to control which members will become primary. Instead, modify the priority option. Only alter the number of votes in exceptional cases. For example, to permit more than seven members.
When possible, all members should have one vote. Changing the number of votes can cause the wrong members to become primary.

To configure a non-voting member, see Configure Non-Voting Replica Set Member.
























mongod --config D:\mongoCluster\shards\replSet1\primary\mongod.conf
mongod --config D:\mongoCluster\shards\replSet1\secondary\node1\mongod.conf
mongod --config D:\mongoCluster\shards\replSet1\secondary\node2\mongod.conf
mongod --configsvr --dbpath D:\mongoCluster\configServer\configdb1 --port 27020
mongos --configdb 172.18.128.79:27020 --port 27030 


-- One time Job

mongo 172.18.128.79:27017
rs.initiate()
rs.add('172.18.128.79:27018');
rs.add('172.18.128.79:27019');

mongo 172.18.128.79:27030
sh.addShard("replSet1/172.18.128.79:27017");  


D:\mongoCluster\shards\replSet1\primary\mongod.conf

dbpath=D:\mongoData
logpath=D:\mongoData\log\mongodb.log
logappend=true
nojournal = false
port = 27017
bind_ip = 172.18.128.79
replSet =  replSet1

D:\mongoCluster\shards\replSet1\secondary\node1\mongod.conf 

dbpath=D:\mongoData
logpath=D:\mongoData\log\mongodb.log
logappend=true
nojournal = false
port = 27018
bind_ip = 172.18.128.79
replSet =  replSet1


D:\mongoCluster\shards\replSet1\secondary\node2\mongod.conf

dbpath=D:\mongoData
logpath=D:\mongoData\log\mongodb.log
logappend=true
nojournal = false
port = 27019
bind_ip = 172.18.128.79
replSet =  replSet1
